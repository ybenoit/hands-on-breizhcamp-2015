{
  "metadata" : {
    "name" : "KMeansClusteringSolution",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# KMeans Clustering"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###By default a SparkContext is available in the variable ***sparkContext***. We put it in a variable named ***sc*** for more simplicity"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sc = sparkContext",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:7: error: not found: value sparkContext\n       val sc = sparkContext\n                ^\n"
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We import all the libraries we will need in this notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.clustering.KMeansModel",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.clustering.KMeansModel\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We declare several functions that will be used just after"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The featureEngineering function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def featureEngineering(data : RDD[String]): RDD[Vector] = {\n  data.map(line => {\n\n  val values = line.replaceAll(\"\\\"\",\"\").split('$')\n\n  val pClass = values(0).toDouble\n\n    val age = values(4) match {\n      case \"NA\" => 28d\n      case l => l.toDouble\n    }\n    val sibsp = values(5).toDouble\n    val parch = values(6).toDouble\n    val fair = values(8) match {\n      case \"NA\" => 14.45\n      case l => l.toDouble\n    }\n\n    val numericalData = Array(pClass, age, fair, sibsp, parch)\n\n    Vectors.dense(numericalData)\n  })\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureEngineering: (data: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The clustersInfo function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Gives the centroids information of a KMeansModel\n * @param model A KMeansModel from the method KMeans.train()\n * @return The centroids and the proportion of survivors\n */\ndef clustersInfo(model: KMeansModel) = {\n\n  val centroids = model.clusterCenters\n\n  centroids.foreach(l => println(s\"Class: ${l(0).toInt}, Age: ${l(1).toInt}, Fair: ${l(2).toInt}\"))\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "clustersInfo: (model: org.apache.spark.mllib.clustering.KMeansModel)Unit\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###Your turn now ! Just follow the instructions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Loading data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : read file ./data/data_titanic.csv\nval rawData = sc.textFile(\"./data/data_titanic.csv\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.rdd.RDD[String] = ./data/data_titanic.csv MapPartitionsRDD[1] at textFile at <console>:42\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "./data/data_titanic.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:42"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Feature Engineering"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use the featureEngineering method to get the cleaned data\nval cleanData: RDD[Vector] = featureEngineering(rawData)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "cleanData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[3] at map at <console>:40\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[3] at map at &lt;console&gt;:40"
      },
      "output_type" : "execute_result",
      "execution_count" : 13
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Modelling"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : Train a KMeans model on the data set\nval model = KMeans.train(cleanData, 5, 50)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@1fb853e9\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.mllib.clustering.KMeansModel@1fb853e9"
      },
      "output_type" : "execute_result",
      "execution_count" : 17
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Inspect centroid of each cluster"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : For each cluster, print the centroid information. \n// You can use the clustersInfo method\nprintln(\"Clusters description\")\nval statsPerCluster = clustersInfo(model)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Clusters description\nClass: 2, Age: 45, Fair: 20\nClass: 1, Age: 41, Fair: 512\nClass: 1, Age: 34, Fair: 71\nClass: 2, Age: 23, Fair: 13\nClass: 1, Age: 35, Fair: 195\nstatsPerCluster: Unit = ()\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 19
    } ]
  } ],
  "nbformat" : 4
}