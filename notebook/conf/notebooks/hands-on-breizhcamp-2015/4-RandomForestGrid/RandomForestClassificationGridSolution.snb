{
  "metadata" : {
    "name" : "RandomForestClassificationGridSolution",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Random Forest Classification"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###By default a SparkContext is available in the variable ***sparkContext***. We put it in a variable named ***sc*** for more simplicity"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sc = sparkContext",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3292e284\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.SparkContext@3292e284"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We import all the libraries we will need in this notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We declare several functions that will be used just after"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The featureEngineering function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def featureEngineering(data : RDD[String]): RDD[LabeledPoint] = {\n  data.map(line => {\n\n    val values = line.replaceAll(\"\\\"\",\"\").split('$')\n\n    val label = values(1).toDouble\n\n    val pClass = values(0).toDouble\n    val sex = values(3) match {\n      case \"male\" => 0d\n      case \"female\" => 1d\n    }\n    val age = values(4) match {\n      case \"NA\" => 28d\n      case l => l.toDouble\n    }\n    val sibsp = values(5).toDouble\n    val parch = values(6).toDouble\n    val fair = values(8) match {\n      case \"NA\" => 14.45\n      case l => l.toDouble\n    }\n    val embarked = values(10) match {\n      case \"\" => 0d\n      case \"C\" => 1d\n      case \"Q\" => 2d\n      case \"S\" => 3d\n    }\n\n    val cleanedData = Array(pClass, sex, age, sibsp, parch, fair, embarked)\n\n    LabeledPoint(label, Vectors.dense(cleanedData))\n  })\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureEngineering: (data: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The accuracyRandomForest function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Calculate the accuracy of the given model\n * @param model A RandomForestModel from the method RandomForest.trainClassifier()\n * @param data the data (a RDD[LabeledPoint])\n * @return A tuple giving the accuracy and the confusion matrix\n */\ndef accuracyRandomForest(model: RandomForestModel, data: RDD[LabeledPoint]): Double = {\n\n  val predictionsAndLabels = data.map(l => (model.predict(l.features), l.label))\n\n  val metrics: MulticlassMetrics = new MulticlassMetrics(predictionsAndLabels)\n\n  val accuracy = 100d * metrics.precision\n\n  accuracy\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "accuracyRandomForest: (model: org.apache.spark.mllib.tree.model.RandomForestModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])Double\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The gridSearchRFClassifier function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n   * Perform a Grid Search to find the best parameters for the Random Forest\n   * @param trainSet: RDD[LabeledPoint] - The training set\n   * @param valSet: RDD[LabeledPoint] - The validation set\n   * @param categoricalFeaturesInfo: A Map indicating which features are categorical and how many categories they contain\n   * @param numTreesGrid: The number of trees to train\n   * @param featuresSubsetStrategy: Strategy to select a subset of the features for splitting (use \"auto\")\n   * @param impurityGrid: The impurity measure to select the best feature for splitting (\"entropy\" or \"gini\")\n   * @param maxDepthGrid: The maximum depth of each tree\n   * @param maxBinsGrid: The maximum number of leaves for each tree\n   * @return The best parameters found, in a tuple.\n   */\n  def gridSearchRFClassifier(trainSet: RDD[LabeledPoint],\n                             valSet: RDD[LabeledPoint],\n                             categoricalFeaturesInfo: Map[Int, Int] = Map[Int, Int](),\n                             numTreesGrid: Array[Int] = Array(10),\n                             featuresSubsetStrategy: String = \"auto\",\n                             impurityGrid: Array[String] = Array(\"entropy\"),\n                             maxDepthGrid: Array[Int] = Array(2),\n                             maxBinsGrid: Array[Int] = Array(4)) = {\n\n    val gridSearch =\n\n      for (numTrees <- numTreesGrid;\n           impurity <- impurityGrid;\n           maxDepth <- maxDepthGrid;\n           maxBins <- maxBinsGrid)\n        yield {\n\n          val model = RandomForest.trainClassifier(trainSet, 2, categoricalFeaturesInfo, numTrees, featuresSubsetStrategy, impurity, maxDepth, maxBins)\n          val accuracyVal = accuracyRandomForest(model, valSet)\n\n          ((numTrees, impurity, maxDepth, maxBins), accuracyVal)\n        }\n\n    val params = gridSearch.sortBy(_._2).reverse(0)._1\n    val numTrees = params._1\n    val impurity = params._2\n    val maxDepth = params._3\n    val maxBins = params._4\n\n    (numTrees, impurity, maxDepth, maxBins)\n  }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "gridSearchRFClassifier: (trainSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint], valSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint], categoricalFeaturesInfo: Map[Int,Int], numTreesGrid: Array[Int], featuresSubsetStrategy: String, impurityGrid: Array[String], maxDepthGrid: Array[Int], maxBinsGrid: Array[Int])(Int, String, Int, Int)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###Your turn now ! Just follow the instructions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Loading data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : read file ./data/data_titanic.csv\nval rawData = sc.textFile(\"./data/data_titanic.csv\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.rdd.RDD[String] = ./data/data_titanic.csv MapPartitionsRDD[1] at textFile at <console>:43\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "./data/data_titanic.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:43"
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Feature Engineering"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use the featureEngineering method to get the cleaned data.\n// Be carefull, you will get a RDD[LabeledPoint]\nval cleanData = featureEngineering(rawData)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "cleanData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:41\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[2] at map at &lt;console&gt;:41"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Splitting Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : split the cleaned data in a train, validation and test set (proportions 0.60, 0.15, 0.25) using the 'randomSplit' method\nval Array(trainSet, valSet, testSet) = cleanData.randomSplit(Array(0.6, 0.15, 0.25), seed = 54)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "trainSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = PartitionwiseSampledRDD[3] at randomSplit at <console>:49\nvalSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = PartitionwiseSampledRDD[4] at randomSplit at <console>:49\ntestSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = PartitionwiseSampledRDD[5] at randomSplit at <console>:49\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "PartitionwiseSampledRDD[5] at randomSplit at &lt;console&gt;:49"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Modelling"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#####Tuning parameters"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val categoricalFeaturesInfo = Map(1 -> 2, 6 -> 4)\nval featuresSubsetStrategy = \"auto\"\n// TODO: Implement a grid search to search for the best parameters, using the train and validation sets\n// (You can use the given gridSearchRandomForestClassifier method in tools/Utilities)\nval bestParams = gridSearchRFClassifier(trainSet, valSet,\n  categoricalFeaturesInfo = categoricalFeaturesInfo, numTreesGrid = Array(50, 100),\n  impurityGrid = Array(\"entropy\", \"gini\"), maxDepthGrid = Array(5, 10), maxBinsGrid = Array(50, 100))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map(1 -> 2, 6 -> 4)\nfeaturesSubsetStrategy: String = auto\nbestParams: (Int, String, Int, Int) = (100,gini,10,100)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "(100,gini,10,100)"
      },
      "output_type" : "execute_result",
      "execution_count" : 11
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#####Training the model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : Train a RandomForest model on the training set (Use the best parameters found)\nval dataTrain = sc.union(trainSet, valSet)\nval (numTrees, impurity, maxDepth, maxBins) = bestParams\nval model: RandomForestModel = RandomForest.trainClassifier(dataTrain, 2, categoricalFeaturesInfo, numTrees, featuresSubsetStrategy, impurity, maxDepth, maxBins)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataTrain: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = UnionRDD[542] at union at <console>:57\nnumTrees: Int = 100\nimpurity: String = gini\nmaxDepth: Int = 10\nmaxBins: Int = 100\nmodel: org.apache.spark.mllib.tree.model.RandomForestModel = \nTreeEnsembleModel classifier with 100 trees\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "TreeEnsembleModel classifier with 100 trees\n"
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Prediction and Evaluation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : get the precision for the prediction on the test set\n// You can use the accuracyRandomForest method\nval accuracyTest = accuracyRandomForest(model, testSet)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "accuracyTest: Double = 79.15407854984893\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "79.15407854984893"
      },
      "output_type" : "execute_result",
      "execution_count" : 13
    } ]
  } ],
  "nbformat" : 4
}